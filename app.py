# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l4e-Qs2W5_YSt7Q7XT8X2s7aY-zGinrr
"""

import streamlit as st
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

# Load model and tokenizer
@st.cache_resource
def load_model():
    llama_model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path="aboonaji/llama2finetune-v2",
        quantization_config=BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4"
        )
    )
    llama_tokenizer = AutoTokenizer.from_pretrained("aboonaji/llama2finetune-v2", trust_remote_code=True)
    llama_tokenizer.pad_token = llama_tokenizer.eos_token
    llama_tokenizer.padding_side = "right"
    return llama_model, llama_tokenizer

llama_model, llama_tokenizer = load_model()

# Initialize text generation pipeline
text_generation_pipeline = pipeline(
    task="text-generation",
    model=llama_model,
    tokenizer=llama_tokenizer,
    max_length=300
)

# Title of the web app
st.title("Medical Chatbot")

# Input field for the user prompt
user_prompt = st.text_input("Ask me anything about medical topics:")

# Generate response when user provides a prompt
if user_prompt:
    with st.spinner("Generating response..."):
        # Format the prompt with instruction tokens and generate the model's response
        model_answer = text_generation_pipeline(f"<s>[INST] {user_prompt} [/INST]")
        response = model_answer[0]['generated_text']
        st.subheader("Response from the chatbot:")
        st.write(response)